{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install imblearn\n",
        "# !pip install numpy\n",
        "# !pip install pandas\n",
        "# !pip install tqdm\n",
        "# !pip install scikit-learn\n",
        "# !pip install torch\n",
        "# !pip install flwr\n",
        "# !pip install -U \"flwr[simulation]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# ==================== IMPORTS ====================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
        "import warnings\n",
        "import os\n",
        "import time\n",
        "from scipy import stats\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== ADAPTIVE CONFIGURATION ====================\n",
        "K = 6                    # Number of clients\n",
        "local_epochs = 5         # Local epochs per client\n",
        "T = 10                   # Rrounds \n",
        "batch_size = 128         # Batch size\n",
        "latent_dim = 20\n",
        "lr = 1e-4\n",
        "kl_weight = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdaptiveCTVAE(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim=15, dataset_num=1, use_bce=False):\n",
        "        super(AdaptiveCTVAE, self).__init__()\n",
        "        \n",
        "        self.latent_dim = latent_dim\n",
        "        self.dataset_num = dataset_num\n",
        "        self.use_bce = use_bce\n",
        "        \n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(32, latent_dim * 2)\n",
        "        )\n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(32, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "        # Class embeddings با مقداردهی اولیه مناسب\n",
        "        self.class_embed = nn.Embedding(2, latent_dim)\n",
        "        nn.init.uniform_(self.class_embed.weight, -0.03, 0.03)\n",
        "        self.eps = 1e-8\n",
        "    \n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h.chunk(2, dim=1)\n",
        "        \n",
        "        logvar = torch.clamp(logvar, -6, 6)\n",
        "        \n",
        "        return mu, logvar\n",
        "    \n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "    \n",
        "    def forward(self, x, y):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        \n",
        "        # افزودن embedding کلاس با وزن مناسب\n",
        "        class_emb = self.class_embed(y)\n",
        "        z_cond = z + 0.1 * class_emb\n",
        "        \n",
        "        recon = self.decoder(z_cond)\n",
        "        return recon, mu, logvar, z_cond\n",
        "    \n",
        "    def loss_fn(self, recon, x, mu, logvar, kl_weight=0.1):\n",
        "        # Clamping برای جلوگیری از NaN\n",
        "        recon = torch.clamp(recon, 1e-8, 1 - 1e-8)\n",
        "        \n",
        "        # انتخاب تابع loss بازسازی\n",
        "        if self.use_bce:\n",
        "            recon_loss = nn.BCELoss(reduction='mean')(recon, x)\n",
        "        else:\n",
        "            recon_loss = nn.MSELoss(reduction='mean')(recon, x)\n",
        "        \n",
        "        # محاسبه KL divergence با محافظت\n",
        "        kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        \n",
        "        # وزن‌دهی بر اساس دیتاست\n",
        "        total_loss = recon_loss + kl_weight * kl_loss\n",
        "        \n",
        "        # بررسی NaN\n",
        "        if torch.isnan(total_loss).any():\n",
        "            return torch.tensor(0.0, requires_grad=True, device=x.device)\n",
        "        \n",
        "        return total_loss, recon_loss, kl_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== IMPROVED DATA LOADING ====================\n",
        "def load_iot_data_improved(dataset_number):\n",
        "    \"\"\"Load IoT dataset with intelligent preprocessing\"\"\"\n",
        "    i = str(dataset_number)\n",
        "    \n",
        "    print(f\"\\nLoading Dataset {dataset_number}...\")\n",
        "    \n",
        "    # لیست فایل‌ها\n",
        "    benign_file = f\"dataset/{i}/{i}.benign.csv\"\n",
        "    \n",
        "    # بررسی وجود فایل\n",
        "    if not os.path.exists(benign_file):\n",
        "        raise FileNotFoundError(f\"File not found: {benign_file}\")\n",
        "    \n",
        "    # Load benign data\n",
        "    benign_df = pd.read_csv(benign_file)\n",
        "    \n",
        "    # Load attack files\n",
        "    attack_files = []\n",
        "    \n",
        "    # Gafgyt attacks\n",
        "    gafgyt_types = ['combo', 'junk', 'scan', 'tcp', 'udp']\n",
        "    for attack_type in gafgyt_types:\n",
        "        attack_file = f\"dataset/{i}/{i}.gafgyt.{attack_type}.csv\"\n",
        "        if os.path.exists(attack_file):\n",
        "            attack_files.append(attack_file)\n",
        "    \n",
        "    # Mirai attacks (except for datasets 3 and 7)\n",
        "    if dataset_number not in [3, 7]:\n",
        "        mirai_types = ['scan', 'syn', 'udp', 'ack', 'udpplain']\n",
        "        for attack_type in mirai_types:\n",
        "            attack_file = f\"dataset/{i}/{i}.mirai.{attack_type}.csv\"\n",
        "            if os.path.exists(attack_file):\n",
        "                attack_files.append(attack_file)\n",
        "    \n",
        "    # Process benign data\n",
        "    benign_df = benign_df.dropna()\n",
        "    benign_train, benign_test = train_test_split(benign_df, test_size=0.3, random_state=42)\n",
        "    \n",
        "    # Load and process attack data\n",
        "    attack_dfs = []\n",
        "    for attack_file in attack_files:\n",
        "        try:\n",
        "            df = pd.read_csv(attack_file).dropna()\n",
        "            if len(df) > 0:\n",
        "                attack_dfs.append(df)\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Error loading {attack_file}: {e}\")\n",
        "    \n",
        "    if attack_dfs:\n",
        "        attacks = pd.concat(attack_dfs, ignore_index=True)\n",
        "    else:\n",
        "        attacks = pd.DataFrame()\n",
        "        print(f\"  No attack files found for dataset {dataset_number}\")\n",
        "    \n",
        "    # ===== STRATEGIC DATA SPLITTING =====\n",
        "    X_train = benign_train.drop(columns=['label'], errors='ignore').values\n",
        "    y_train = np.zeros(len(benign_train))\n",
        "    \n",
        "    if len(attacks) > 0:\n",
        "        n_attack_train = min(100000, len(attacks))\n",
        "        attack_train = attacks.sample(n=n_attack_train, random_state=42)\n",
        "        X_attack = attack_train.drop(columns=['label'], errors='ignore').values\n",
        "        \n",
        "        X_train = np.concatenate([X_train, X_attack])\n",
        "        y_train = np.concatenate([y_train, np.ones(len(attack_train))])\n",
        "    \n",
        "    # ایجاد داده تست\n",
        "    X_test = benign_test.drop(columns=['label'], errors='ignore').values\n",
        "    y_test = np.zeros(len(benign_test))\n",
        "    \n",
        "    if len(attacks) > 0:\n",
        "        max_attack_test = 20000\n",
        "        \n",
        "        if 'attack_train' in locals():\n",
        "            remaining_attacks = attacks.drop(attack_train.index)\n",
        "        else:\n",
        "            remaining_attacks = attacks\n",
        "        \n",
        "        if len(remaining_attacks) > max_attack_test:\n",
        "            attack_test = remaining_attacks.sample(n=max_attack_test, random_state=42)\n",
        "        else:\n",
        "            attack_test = remaining_attacks\n",
        "        \n",
        "        X_attack_test = attack_test.drop(columns=['label'], errors='ignore').values\n",
        "        \n",
        "        X_test = np.concatenate([X_test, X_attack_test])\n",
        "        y_test = np.concatenate([y_test, np.ones(len(attack_test))])\n",
        "    \n",
        "    # ===== INTELLIGENT PREPROCESSING =====\n",
        "    print(f\"\\nPreprocessing data...\")\n",
        "    \n",
        "    scaler = MinMaxScaler(feature_range=(0.1, 0.9))\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "    \n",
        "    input_dim = X_train.shape[1]\n",
        "    \n",
        "    # Convert to tensors\n",
        "    X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "    y_train_t = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "    X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    y_test_t = torch.tensor(y_test, dtype=torch.long).to(device)\n",
        "    \n",
        "    return X_train_t, y_train_t, X_test_t, y_test_t, input_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== FEDERATED CLIENT ====================\n",
        "class FLClient:\n",
        "    def __init__(self, cid, X, y, input_dim, latent_dim, dataset_num, use_bce=False):\n",
        "        self.cid = cid\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.dataset_num = dataset_num\n",
        "        \n",
        "        # مدل با تنظیمات ویژه\n",
        "        self.model = AdaptiveCTVAE(input_dim, latent_dim, dataset_num, use_bce).to(device)\n",
        "        \n",
        "        # تنظیم optimizer\n",
        "        self.optimizer = optim.AdamW(self.model.parameters(), lr=2e-4, weight_decay=5e-6)\n",
        "        \n",
        "        # ایجاد DataLoader\n",
        "        dataset = TensorDataset(X, y)\n",
        "        self.loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "        \n",
        "        # ثبت loss\n",
        "        self.train_losses = []\n",
        "    \n",
        "    def train_local(self, global_state_dict, kl_weight=0.1):\n",
        "        \"\"\"Train locally and return update\"\"\"\n",
        "        # بارگذاری مدل سرور\n",
        "        self.model.load_state_dict(global_state_dict)\n",
        "        self.model.train()\n",
        "        \n",
        "        total_loss = 0\n",
        "        recon_loss_total = 0\n",
        "        kl_loss_total = 0\n",
        "        \n",
        "        for epoch in range(local_epochs):\n",
        "            epoch_loss = 0\n",
        "            epoch_recon = 0\n",
        "            epoch_kl = 0\n",
        "            \n",
        "            for batch_x, batch_y in self.loader:\n",
        "                self.optimizer.zero_grad()\n",
        "                \n",
        "                recon, mu, logvar, _ = self.model(batch_x, batch_y)\n",
        "                loss, recon_loss, kl_loss = self.model.loss_fn(recon, batch_x, mu, logvar, kl_weight)\n",
        "                \n",
        "                # بررسی NaN\n",
        "                if torch.isnan(loss).any() or loss.item() > 1000:\n",
        "                    print(f\"Client {self.cid}: Invalid loss {loss.item():.4f}, skipping\")\n",
        "                    continue\n",
        "                \n",
        "                loss.backward()\n",
        "                \n",
        "                # Gradient clipping محکم\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n",
        "                \n",
        "                # بررسی gradient\n",
        "                valid_grad = True\n",
        "                for param in self.model.parameters():\n",
        "                    if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
        "                        valid_grad = False\n",
        "                        break\n",
        "                \n",
        "                if valid_grad:\n",
        "                    self.optimizer.step()\n",
        "                    epoch_loss += loss.item()\n",
        "                    epoch_recon += recon_loss.item()\n",
        "                    epoch_kl += kl_loss.item()\n",
        "            \n",
        "            if len(self.loader) > 0:\n",
        "                avg_epoch_loss = epoch_loss / len(self.loader)\n",
        "                avg_recon = epoch_recon / len(self.loader)\n",
        "                avg_kl = epoch_kl / len(self.loader)\n",
        "                \n",
        "                total_loss += avg_epoch_loss\n",
        "                recon_loss_total += avg_recon\n",
        "                kl_loss_total += avg_kl\n",
        "                \n",
        "                self.train_losses.append(avg_epoch_loss)\n",
        "        \n",
        "        # محاسبه update\n",
        "        local_state = self.model.state_dict()\n",
        "        update = {}\n",
        "        for key in global_state_dict.keys():\n",
        "            update[key] = local_state[key].float() - global_state_dict[key].float()\n",
        "        \n",
        "        avg_total_loss = total_loss / local_epochs if local_epochs > 0 else 0\n",
        "        avg_recon_loss = recon_loss_total / local_epochs if local_epochs > 0 else 0\n",
        "        avg_kl_loss = kl_loss_total / local_epochs if local_epochs > 0 else 0\n",
        "        \n",
        "        return update, avg_total_loss, avg_recon_loss, avg_kl_loss, len(self.X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FLServer:\n",
        "    def __init__(self, input_dim, latent_dim, dataset_num, use_bce=False):\n",
        "        self.global_model = AdaptiveCTVAE(input_dim, latent_dim, dataset_num, use_bce).to(device)\n",
        "        self.clients = []\n",
        "        self.dataset_num = dataset_num\n",
        "        self.round_losses = []\n",
        "        self.round_recon_losses = []\n",
        "        self.round_kl_losses = []\n",
        "        \n",
        "        # مقداردهی اولیه هوشمند\n",
        "        self._initialize_model()\n",
        "    \n",
        "    def _initialize_model(self):\n",
        "        \"\"\"Initialize model with smart pre-training\"\"\"\n",
        "        lr_init = 1e-4\n",
        "        \n",
        "        temp_optimizer = optim.AdamW(self.global_model.parameters(), lr=lr_init, weight_decay=1e-5)\n",
        "        \n",
        "        # داده‌های dummy با توزیع مناسب\n",
        "        dummy_X = torch.randn(200, self.global_model.encoder[0].in_features).to(device)\n",
        "        dummy_X = torch.clamp(dummy_X, 0.1, 0.9)\n",
        "        dummy_y = torch.randint(0, 2, (200,)).to(device)\n",
        "        \n",
        "        print(\"  Initializing model with pre-training...\")\n",
        "        for init_step in range(10):\n",
        "            temp_optimizer.zero_grad()\n",
        "            recon, mu, logvar, _ = self.global_model(dummy_X, dummy_y)\n",
        "            loss, _, _ = self.global_model.loss_fn(recon, dummy_X, mu, logvar)\n",
        "            \n",
        "            if not torch.isnan(loss).any():\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.global_model.parameters(), max_norm=0.3)\n",
        "                temp_optimizer.step()\n",
        "            \n",
        "            if init_step % 2 == 0:\n",
        "                print(f\"    Pre-train step {init_step+1}/10: loss={loss.item():.6f}\")\n",
        "    \n",
        "    def add_client(self, client):\n",
        "        self.clients.append(client)\n",
        "    \n",
        "    def train_round(self, round_num, kl_weight=0.1):\n",
        "        print(f\"  Round {round_num}: Training clients...\")\n",
        "        \n",
        "        global_state = self.global_model.state_dict()\n",
        "        \n",
        "        # جمع‌آوری updates از کلاینت‌ها\n",
        "        updates = []\n",
        "        losses = []\n",
        "        recon_losses = []\n",
        "        kl_losses = []\n",
        "        client_samples = []\n",
        "        \n",
        "        for i, client in enumerate(self.clients):\n",
        "            try:\n",
        "                update, loss, recon_loss, kl_loss, n_samples = client.train_local(\n",
        "                    global_state, kl_weight\n",
        "                )\n",
        "                \n",
        "                # بررسی کیفیت update\n",
        "                update_valid = True\n",
        "                update_norm = 0.0\n",
        "                \n",
        "                for key in update.keys():\n",
        "                    if torch.isnan(update[key]).any() or torch.isinf(update[key]).any():\n",
        "                        update_valid = False\n",
        "                        break\n",
        "                    update_norm += torch.norm(update[key]).item()\n",
        "                \n",
        "                if update_valid and loss < 100:  # loss معقول\n",
        "                    updates.append(update)\n",
        "                    losses.append(loss)\n",
        "                    recon_losses.append(recon_loss)\n",
        "                    kl_losses.append(kl_loss)\n",
        "                    client_samples.append(n_samples)\n",
        "                    \n",
        "                else:\n",
        "                    zero_update = {key: torch.zeros_like(global_state[key]) for key in global_state.keys()}\n",
        "                    updates.append(zero_update)\n",
        "                    losses.append(1.0)\n",
        "                    recon_losses.append(0.5)\n",
        "                    kl_losses.append(0.5)\n",
        "                    client_samples.append(n_samples)\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"    Client {i+1}: Error - {str(e)[:50]}\")\n",
        "                zero_update = {key: torch.zeros_like(global_state[key]) for key in global_state.keys()}\n",
        "                updates.append(zero_update)\n",
        "                losses.append(2.0)\n",
        "                recon_losses.append(1.0)\n",
        "                kl_losses.append(1.0)\n",
        "                client_samples.append(1000)\n",
        "        \n",
        "        # Federated Averaging با وزن‌دهی\n",
        "        total_samples = sum(client_samples)\n",
        "        if total_samples == 0:\n",
        "            total_samples = 1\n",
        "        \n",
        "        avg_update = {}\n",
        "        for key in updates[0].keys():\n",
        "            weighted_sum = torch.zeros_like(updates[0][key], dtype=torch.float32)\n",
        "            for i in range(len(updates)):\n",
        "                weight = client_samples[i] / total_samples\n",
        "                weighted_sum += weight * updates[i][key]\n",
        "            avg_update[key] = weighted_sum\n",
        "        \n",
        "        # اعمال update با momentum تطبیقی\n",
        "        momentum = 0.1\n",
        "        \n",
        "        new_state = {}\n",
        "        for key in global_state.keys():\n",
        "            current = global_state[key].float()\n",
        "            update = avg_update[key].float()\n",
        "            \n",
        "            # محدود کردن بزرگی update\n",
        "            update = torch.clamp(update, -0.2, 0.2)\n",
        "            \n",
        "            # اعمال momentum\n",
        "            new_state[key] = (current + momentum * current + (1 - momentum) * update).to(global_state[key].dtype)\n",
        "        \n",
        "        # بررسی state جدید\n",
        "        state_valid = True\n",
        "        for key in new_state.keys():\n",
        "            if torch.isnan(new_state[key]).any() or torch.isinf(new_state[key]).any():\n",
        "                state_valid = False\n",
        "                print(f\"    Invalid state for {key}, keeping old state\")\n",
        "                break\n",
        "        \n",
        "        if state_valid:\n",
        "            self.global_model.load_state_dict(new_state)\n",
        "        \n",
        "        # محاسبه میانگین loss\n",
        "        avg_loss = np.mean(losses) if losses else 1.0\n",
        "        avg_recon = np.mean(recon_losses) if recon_losses else 0.5\n",
        "        avg_kl = np.mean(kl_losses) if kl_losses else 0.5\n",
        "        \n",
        "        self.round_losses.append(avg_loss)\n",
        "        self.round_recon_losses.append(avg_recon)\n",
        "        self.round_kl_losses.append(avg_kl)\n",
        "        \n",
        "        return avg_loss, avg_recon, avg_kl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== ENHANCED EVALUATION ====================\n",
        "def evaluate_model_enhanced(model, X_train, y_train, X_test, y_test, dataset_num):\n",
        "    \"\"\"Evaluate the trained model with enhanced metrics\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"MODEL EVALUATION\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # تابع استخراج features\n",
        "    def extract_features(X, y, batch_size=4096):\n",
        "        features = []\n",
        "        labels = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(X), batch_size):\n",
        "                end = min(i + batch_size, len(X))\n",
        "                batch_x = X[i:end]\n",
        "                batch_y = y[i:end]\n",
        "                \n",
        "                _, _, _, z = model(batch_x, batch_y)\n",
        "                z_np = z.cpu().numpy()\n",
        "                \n",
        "                # پاکسازی NaN\n",
        "                if np.isnan(z_np).any():\n",
        "                    z_np = np.nan_to_num(z_np, nan=0.0)\n",
        "                \n",
        "                features.append(z_np)\n",
        "                labels.append(batch_y.cpu().numpy())\n",
        "        \n",
        "        features_array = np.concatenate(features)\n",
        "        labels_array = np.concatenate(labels)\n",
        "        \n",
        "        return features_array, labels_array\n",
        "    \n",
        "    # آموزش RandomForest پیشرفته\n",
        "    print(\" Training RandomForest classifier...\")\n",
        "    \n",
        "    rf_params = {\n",
        "        'n_estimators': 100,\n",
        "        'max_depth': 15,\n",
        "        'min_samples_split': 10,\n",
        "        'min_samples_leaf': 5,\n",
        "        'max_features': 'sqrt',\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "    \n",
        "    rf = RandomForestClassifier(**rf_params)\n",
        "    \n",
        "    z_train, y_train_clean = extract_features(X_train, y_train)\n",
        "    z_test, y_test_clean = extract_features(X_test, y_test)\n",
        "    rf.fit(z_train, y_train_clean)\n",
        "\n",
        "    # پیش‌بینی\n",
        "    print(\"\\n Making predictions...\")\n",
        "    y_pred = rf.predict(z_test)\n",
        "    y_true = y_test_clean\n",
        "    \n",
        "    # محاسبه معیارها\n",
        "    acc = accuracy_score(y_true, y_pred) * 100\n",
        "    f1 = f1_score(y_true, y_pred) * 100\n",
        "    precision = precision_score(y_true, y_pred) * 100\n",
        "    recall = recall_score(y_true, y_pred) * 100\n",
        "    \n",
        "    # ماتریس درهم‌ریختگی\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    if cm.shape == (2, 2):\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        \n",
        "        # محاسبه نرخ‌ها\n",
        "        fpr = fp / (fp + tn) * 100 if (fp + tn) > 0 else 0\n",
        "        fnr = fn / (fn + tp) * 100 if (fn + tp) > 0 else 0\n",
        "        tpr = tp / (tp + fn) * 100 if (tp + fn) > 0 else 0\n",
        "        tnr = tn / (tn + fp) * 100 if (tn + fp) > 0 else 0\n",
        "    else:\n",
        "        tn = fp = fn = tp = 0\n",
        "        fpr = fnr = tpr = tnr = 0\n",
        "    \n",
        "    # نمایش نتایج\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    return acc, f1, cm, precision, recall, fpr, fnr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== MAIN FEDERATED TRAINING ====================\n",
        "def run_federated_ctvae_complete(dataset_number):\n",
        "    \"\"\"Run complete federated CTVAE training\"\"\"\n",
        "    start_time = time.time()\n",
        "       \n",
        "    # 1. بارگذاری داده‌ها\n",
        "    print(f\"\\n[1/3] Loading data...\")\n",
        "    try:\n",
        "        X_train, y_train, X_test, y_test, input_dim = load_iot_data_improved(dataset_number)\n",
        "    except Exception as e:\n",
        "        print(f\" Error loading data: {e}\")\n",
        "        return 0, 0, 0\n",
        "    \n",
        "    # 2. راه‌اندازی Federated Learning\n",
        "    print(f\"\\n[2/3] Initializing Federated Learning...\")\n",
        "    \n",
        "    # ایجاد سرور با تنظیمات ویژه\n",
        "    server = FLServer(input_dim, latent_dim, dataset_number, False)\n",
        "    \n",
        "    # تقسیم داده بین کلاینت‌ها\n",
        "    n_total_samples = min(40000, len(X_train))  # افزایش حجم داده\n",
        "    \n",
        "    y_np = y_train.cpu().numpy()\n",
        "    benign_idx = np.where(y_np == 0)[0]\n",
        "    attack_idx = np.where(y_np == 1)[0]\n",
        "    \n",
        "    # برای دیتاست ۶، استفاده از داده‌های بیشتر\n",
        "    \n",
        "    n_per_client = n_total_samples // K\n",
        "    \n",
        "    n_per_class_per_client = n_per_client // 2\n",
        "    \n",
        "    for k in range(K):\n",
        "        # نمونه‌گیری متعادل\n",
        "        if len(benign_idx) >= n_per_class_per_client and len(attack_idx) >= n_per_class_per_client:\n",
        "            client_benign = np.random.choice(benign_idx, n_per_class_per_client, replace=False)\n",
        "            client_attack = np.random.choice(attack_idx, n_per_class_per_client, replace=False)\n",
        "            \n",
        "            client_idx = np.concatenate([client_benign, client_attack])\n",
        "            np.random.shuffle(client_idx)\n",
        "            \n",
        "            client_X = X_train[client_idx]\n",
        "            client_y = y_train[client_idx]\n",
        "            \n",
        "            # ایجاد کلاینت\n",
        "            client = FLClient(k, client_X, client_y, input_dim, \n",
        "                            latent_dim, dataset_number, False)\n",
        "            server.add_client(client)\n",
        "            \n",
        "        else:\n",
        "            client_idx = np.random.choice(len(X_train), min(n_per_client, len(X_train)), replace=False)\n",
        "            client_X = X_train[client_idx]\n",
        "            client_y = y_train[client_idx]\n",
        "            \n",
        "            client = FLClient(k, client_X, client_y, input_dim,\n",
        "                            latent_dim, dataset_number, False)\n",
        "            server.add_client(client)\n",
        "    \n",
        "    # 3. آموزش فدرال\n",
        "    print(f\"\\n[3/3] Federated Training ({T} rounds)...\")\n",
        "    \n",
        "    round_losses = []\n",
        "    round_recon_losses = []\n",
        "    round_kl_losses = []\n",
        "    \n",
        "    for round_num in range(1, T + 1):\n",
        "        print(f\"\\n  Round {round_num}/{T}\")\n",
        "        \n",
        "        # آموزش راند\n",
        "        loss, recon_loss, kl_loss = server.train_round(round_num, kl_weight)\n",
        "        round_losses.append(loss)\n",
        "        round_recon_losses.append(recon_loss)\n",
        "        round_kl_losses.append(kl_loss)\n",
        "    \n",
        "    print(f\"\\n Training completed!\")\n",
        "    print(f\"   Final loss: {round_losses[-1]:.6f}\")\n",
        "    print(f\"   Training time: {time.time() - start_time:.1f} seconds\")\n",
        "    \n",
        "    # 4. ارزیابی نهایی\n",
        "   \n",
        "    acc, f1, cm, precision, recall, fpr, fnr = evaluate_model_enhanced(\n",
        "        server.global_model, X_train, y_train, X_test, y_test, dataset_number\n",
        "    )\n",
        "    \n",
        "    # مقایسه با baseline\n",
        "    baseline_acc = {1: 93.0, 2: 95.0, 3: 93.9, 4: 91.8, 5: 93.1, 6: 94.5, 7: 96.6, 8: 100.0, 9: 98.9}\n",
        "    baseline_f1 = {1: 90.9, 2: 93.5, 3: 92.2, 4: 89.5, 5: 91.1, 6: 93.3, 7: 95.5, 8: 100.0, 9: 99.0}\n",
        "    \n",
        "    acc_diff = acc - baseline_acc[dataset_number]\n",
        "    f1_diff = f1 - baseline_f1[dataset_number]\n",
        "    \n",
        "    print(f\"\\nDataset: IoT-{dataset_number}\")\n",
        "    print(f\"Number of Clients: {K}\")\n",
        "    print(f\"Number of Epochs: {local_epochs}\")\n",
        "    print(f\"Number of Rounds: {T}\")\n",
        "    print(f\"Batch Size: {batch_size}\")\n",
        "    print(f\"Latent Dim: {latent_dim}\")\n",
        "    print(f\"KL Weight: {kl_weight}\")\n",
        "\n",
        "    print(\"\\n--- CTVAE Results ---\")\n",
        "    print(f\"Accuracy: {baseline_acc[dataset_number]:>11.1f}\")\n",
        "    print(f\"F1-Score: {baseline_f1[dataset_number]:>11.1f}\")\n",
        "\n",
        "    print(\"\\n--- F-CTVAE Results ---\")\n",
        "    print(f\"Accuracy: {acc:>11.2f} ({acc_diff:+.2f})\")\n",
        "    print(f\"F1-Score: {f1:>11.2f} ({f1_diff:+.2f})\")\n",
        "    print(\"\\nConfusion Matrix:\\n\", cm)\n",
        "    \n",
        "    return acc, f1, acc_diff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "══════════════════════════════════════════════════════════════════════\n",
            "F-CTVAE: Federated Constrained & Transformed VAE\n",
            "══════════════════════════════════════════════════════════════════════\n",
            "\n",
            "[1/3] Loading data...\n",
            "\n",
            "Loading Dataset 1...\n",
            "\n",
            "Preprocessing data...\n",
            "\n",
            "[2/3] Initializing Federated Learning...\n",
            "  Initializing model with pre-training...\n",
            "    Pre-train step 1/10: loss=0.155191\n",
            "    Pre-train step 3/10: loss=0.154255\n",
            "    Pre-train step 5/10: loss=0.152844\n",
            "    Pre-train step 7/10: loss=0.152597\n",
            "    Pre-train step 9/10: loss=0.152248\n",
            "\n",
            "[3/3] Federated Training (10 rounds)...\n",
            "\n",
            "  Round 1/10\n",
            "  Round 1: Training clients...\n",
            "\n",
            "  Round 2/10\n",
            "  Round 2: Training clients...\n",
            "\n",
            "  Round 3/10\n",
            "  Round 3: Training clients...\n",
            "\n",
            "  Round 4/10\n",
            "  Round 4: Training clients...\n",
            "\n",
            "  Round 5/10\n",
            "  Round 5: Training clients...\n",
            "\n",
            "  Round 6/10\n",
            "  Round 6: Training clients...\n",
            "\n",
            "  Round 7/10\n",
            "  Round 7: Training clients...\n",
            "\n",
            "  Round 8/10\n",
            "  Round 8: Training clients...\n",
            "\n",
            "  Round 9/10\n",
            "  Round 9: Training clients...\n",
            "\n",
            "  Round 10/10\n",
            "  Round 10: Training clients...\n",
            "\n",
            " Training completed!\n",
            "   Final loss: 0.023385\n",
            "   Training time: 253.5 seconds\n",
            "\n",
            "============================================================\n",
            "MODEL EVALUATION\n",
            "============================================================\n",
            " Training RandomForest classifier...\n",
            "\n",
            " Making predictions...\n",
            "\n",
            "============================================================\n",
            "EVALUATION RESULTS\n",
            "============================================================\n",
            "\n",
            "Dataset: IoT-1\n",
            "Number of Clients: 6\n",
            "Number of Epochs: 5\n",
            "Number of Rounds: 10\n",
            "Batch Size: 128\n",
            "Latent Dim: 20\n",
            "KL Weight: 0.5\n",
            "\n",
            "--- CTVAE Results ---\n",
            "Accuracy:        93.0\n",
            "F1-Score:        90.9\n",
            "\n",
            "--- F-CTVAE Results ---\n",
            "Accuracy:       99.68 (+6.68)\n",
            "F1-Score:       99.73 (+8.83)\n",
            "\n",
            "Confusion Matrix:\n",
            " [[14782    83]\n",
            " [   27 19973]]\n"
          ]
        }
      ],
      "source": [
        "dataset_num = 1         # from 1 to 9\n",
        "\n",
        "print(\"\\n\" + \"═\"*70)\n",
        "print(\"F-CTVAE: Federated Constrained & Transformed VAE\")\n",
        "print(\"═\"*70)\n",
        "\n",
        "try:\n",
        "    acc, f1, acc_diff = run_federated_ctvae_complete(dataset_num)\n",
        "        \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\n Error: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n Unexpected Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
